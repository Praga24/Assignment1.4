1. Characteristics of Big data :

   Volume – Size of data plays very crucial role in determining value out of data.
   Variety – Variety refers to heterogeneous sources and the nature of data, both structured and unstructured.
   Velocity – The term 'velocity' refers to the speed of generation of data. 
              How fast the data is generated and processed to meet the demands, determines real potential in the data.
              Big Data Velocity deals with the speed at which data flows in from sources like business processes, application logs,                  networks and social media sites, sensors, Mobile devices, etc. 
  Variability – This refers to the inconsistency which can be shown by the data at times, 
                thus hampering the process of being able to handle and manage the data effectively.



2. Possible solutions to handle Big data :
   
   There are 2 possible solutions to handle big data:
   
    Scale Up: We can Scale Up the system by
• Increasing the configuration of a single system
• Like disk capacity, RAM, data transfer speed
• Complex, costly, and time consuming process
   
    Scale Out: We can Scale out the system by
• Using multiple commodity machines and distribute the load of
storage/processing among them
• Economical and quick to implement as it focuses on distribution of load
• Instead of having a single system with 10 TB of storage and 80 GB of RAM,
use 40 machines with 256 GB of storage and 2 GB of RAM



3. Differences between scaling up and scaling out :
  
  Scaling up generally refers to purchasing and installing a more capable central control or piece of hardware. For example, when a project’s input/output demands start to push against the limits of an individual server, a scaling up approach would be to buy a more capable server with more processing capacity and RAM.
  
  By contrast, scaling out means linking together other lower-performance machines to collectively do the work of a much more advanced one. With these types of distributed setups, it's easy to handle a larger workload by running data through different system trajectories.
